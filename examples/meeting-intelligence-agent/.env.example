# Local llama.cpp server settings
MIA_LOCAL_BASE_URL=http://localhost:8080/v1
MIA_LOCAL_MODEL=local
MIA_LOCAL_CTX_SIZE=32768
MIA_LOCAL_GPU_LAYERS=99
